{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HisdiRSG7RSs"
      },
      "source": [
        "**Alunos:**\\\n",
        "*Bruno Berndt Lima - 12542550*\\\n",
        "*Fernando Gonçalves Campos - 12542352*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dOdr6G1C7Zub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torcheval\n",
        "# !pip install caserecommender"
      ],
      "metadata": {
        "id": "FxYTXFZr7uAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w746h-Pg7RSv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torcheval\n",
        "import torcheval.metrics\n",
        "import torcheval.metrics.regression\n",
        "\n",
        "from caserec.recommenders.item_recommendation.userknn import UserKNN\n",
        "from caserec.recommenders.rating_prediction.userknn import UserKNN as UserKNN_ratings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from typing import Callable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wECOgHWr7RSw"
      },
      "outputs": [],
      "source": [
        "# WORKING_DIR: str = '.'\n",
        "WORKING_DIR: str = '/content/drive/MyDrive/Trabalho Sistemas de Recomendação'\n",
        "\n",
        "DATASET_DIR: str = f'{WORKING_DIR}/dataset'\n",
        "\n",
        "RATINGS_FILE: str = f'{DATASET_DIR}/ratings_sample.csv'\n",
        "\n",
        "EMBEDDINGS_DIR: str = f'{WORKING_DIR}/embeddings'\n",
        "\n",
        "if not os.path.isdir(EMBEDDINGS_DIR):\n",
        "    os.mkdir(EMBEDDINGS_DIR)\n",
        "\n",
        "MODELS_DIR: str = f'{WORKING_DIR}/models'\n",
        "\n",
        "if not os.path.isdir(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoRHzU0r7RSx"
      },
      "outputs": [],
      "source": [
        "# For similarity calculation\n",
        "USER_ORIGINAL_RATINGS_WEIGHT: float = 15\n",
        "USER_RELATIVE_RATINGS_WEIGHT: float = 0\n",
        "\n",
        "ITEM_ORIGINAL_RATINGS_WEIGHT: float = 15\n",
        "ITEM_RELATIVE_RATINGS_WEIGHT: float = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwCQsNrD7RSx"
      },
      "outputs": [],
      "source": [
        "TEST_SIZE: float = 0.1\n",
        "TRAIN_TEST_SPLIT_SEED: int = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9cgjZyJ7RSy"
      },
      "outputs": [],
      "source": [
        "USER_EMBEDDINGS_SIZE: int = 1024 # In the range [1, n_users]\n",
        "ITEM_EMBEDDINGS_SIZE: int = 400 # In the range [1, n_items]\n",
        "\n",
        "USER_EMBEDDINGS_NAME: str = f'user_pca{USER_EMBEDDINGS_SIZE}'\n",
        "ITEM_EMBEDDINGS_NAME: str = f'item_pca{ITEM_EMBEDDINGS_SIZE}'\n",
        "\n",
        "USER_EMBEDDINGS_FILE: str = f'{EMBEDDINGS_DIR}/{USER_EMBEDDINGS_NAME}.npy'\n",
        "ITEM_EMBEDDINGS_FILE: str = f'{EMBEDDINGS_DIR}/{ITEM_EMBEDDINGS_NAME}.npy'\n",
        "\n",
        "FORCE_NEW_EMBEDDINGS: bool = False\n",
        "SAVE_EMBEDDINGS: bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86BoTZwe7RSy"
      },
      "outputs": [],
      "source": [
        "TRAIN_BATCH_SIZE: int = 256\n",
        "TEST_BATCH_SIZE: int = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTCkweto7RSy"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS: int = 30\n",
        "LEARNING_RATE: float = 0.0005\n",
        "TOWERS_FINAL_SIZE: int = 512\n",
        "DROPOUT: float = 0.15\n",
        "\n",
        "USE_TQDM: bool = True\n",
        "VERBOSE_TRAINNING: bool = True\n",
        "\n",
        "MODEL_NAME: str = f'two_tower-{USER_EMBEDDINGS_NAME}-{ITEM_EMBEDDINGS_NAME}'\n",
        "\n",
        "MODEL_FILE: str = f'{MODELS_DIR}/{MODEL_NAME}.pt'\n",
        "\n",
        "FORCE_TRAINNING: bool = True\n",
        "SAVE_MODEL: bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnhhK5Kz7RSy"
      },
      "outputs": [],
      "source": [
        "PYTORCH_DEVICE: str = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "print(PYTORCH_DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc7LeqiC7RSz"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxuz0Dbu7RSz"
      },
      "outputs": [],
      "source": [
        "ratings: pd.DataFrame = pd.read_csv(RATINGS_FILE, names=['userId', 'itemId', 'rating', 'timestamp'], header=0)\n",
        "ratings: pd.DataFrame = ratings[['userId', 'itemId', 'rating']]\n",
        "\n",
        "map_users: dict[int,int] = {user: idx for idx, user in enumerate(ratings['userId'].unique())}\n",
        "map_items: dict[int,int] = {item: idx for idx, item in enumerate(ratings['itemId'].unique())}\n",
        "\n",
        "ratings['userId'] = ratings['userId'].map(map_users)\n",
        "ratings['itemId'] = ratings['itemId'].map(map_items)\n",
        "\n",
        "ratings = ratings.sort_values(['userId', 'itemId'])\n",
        "ratings['rating'] /= 5\n",
        "\n",
        "existing_users: pd.CategoricalIndex = pd.CategoricalIndex(ratings['userId'].unique())\n",
        "existing_items: pd.CategoricalIndex = pd.CategoricalIndex(ratings['itemId'].unique())\n",
        "\n",
        "ratings['userId'] = ratings['userId'].astype('category').cat.set_categories(existing_users)\n",
        "ratings['itemId'] = ratings['itemId'].astype('category').cat.set_categories(existing_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b087nBP7RS0"
      },
      "outputs": [],
      "source": [
        "print('ratings')\n",
        "display(ratings.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAvRHGVZ7RS0"
      },
      "outputs": [],
      "source": [
        "print(f'number of ratings:  {len(ratings)}')\n",
        "print(f'number of users:  {len(existing_users)}')\n",
        "print(f'number of items:  {len(existing_items)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFPVypou7RS0"
      },
      "outputs": [],
      "source": [
        "train_ratings: pd.DataFrame\n",
        "test_ratings: pd.DataFrame\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size = TEST_SIZE, random_state = TRAIN_TEST_SPLIT_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FalbBl2v7RS0"
      },
      "source": [
        "### Creating pseudo-embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saJAavUf7RS0"
      },
      "source": [
        "#### Calculating similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoF2Shoq7RS0"
      },
      "outputs": [],
      "source": [
        "# Converts the user ratings into the list format for each user\n",
        "buffer_df: pd.DataFrame = ratings[['userId', 'itemId', 'rating']].copy()\n",
        "buffer_df = buffer_df.pivot_table(index='userId', columns='itemId', values='rating', observed=False, fill_value=0)\n",
        "buffer_df = buffer_df.reindex(index=existing_users.to_numpy(), columns=existing_items.to_numpy(), fill_value=0)\n",
        "\n",
        "buffer_df = buffer_df.sort_index()\n",
        "\n",
        "train_user_ratings: np.ndarray = buffer_df.to_numpy()\n",
        "print(train_user_ratings.shape)\n",
        "\n",
        "del buffer_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGEN2Fju7RS0"
      },
      "outputs": [],
      "source": [
        "def get_similarity_and_bias(ratings: np.ndarray, original_ratings_weight: float = 1, relative_ratings_weight: float = 1) -> tuple[np.ndarray, np.ndarray]:\n",
        "    total_weight: float = original_ratings_weight + relative_ratings_weight\n",
        "    original_ratings_weight: float = 0.5\n",
        "    relative_ratings_weight: float = 0.5\n",
        "    if total_weight != 0:\n",
        "        original_ratings_weight = original_ratings_weight / total_weight\n",
        "        relative_ratings_weight = relative_ratings_weight / total_weight\n",
        "\n",
        "    ratings: np.ndarray = ratings.copy()\n",
        "\n",
        "    relative_ratings: np.ma.MaskedArray = np.ma.masked_array(ratings.copy(), ratings == 0)\n",
        "    bias: np.ndarray = np.ma.mean(relative_ratings, axis=1).filled(0)\n",
        "    relative_ratings -= bias[:, np.newaxis]\n",
        "    relative_ratings += 1\n",
        "\n",
        "    rating_similarity: np.ndarray = cosine_similarity(ratings, ratings)\n",
        "    relative_rating_similarity: np.ndarray = cosine_similarity(relative_ratings.filled(0), relative_ratings.filled(0))\n",
        "\n",
        "    np.fill_diagonal(rating_similarity, 1)\n",
        "    np.fill_diagonal(relative_rating_similarity, 1)\n",
        "\n",
        "    similarity = (original_ratings_weight * rating_similarity) + (relative_ratings_weight * relative_rating_similarity)\n",
        "    np.fill_diagonal(similarity, 1)\n",
        "\n",
        "    return similarity, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEYQwhd_7RS1"
      },
      "outputs": [],
      "source": [
        "user_similarity: np.ndarray\n",
        "user_bias: np.ndarray\n",
        "user_similarity, user_bias = get_similarity_and_bias(\n",
        "    ratings = train_user_ratings,\n",
        "    original_ratings_weight = USER_ORIGINAL_RATINGS_WEIGHT,\n",
        "    relative_ratings_weight = USER_RELATIVE_RATINGS_WEIGHT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-UisXgx7RS1"
      },
      "outputs": [],
      "source": [
        "item_similarity: np.ndarray\n",
        "item_bias: np.ndarray\n",
        "item_similarity, item_bias = get_similarity_and_bias(\n",
        "    ratings = train_user_ratings.T,\n",
        "    original_ratings_weight = ITEM_ORIGINAL_RATINGS_WEIGHT,\n",
        "    relative_ratings_weight = ITEM_RELATIVE_RATINGS_WEIGHT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_gIb8CR7RS1"
      },
      "outputs": [],
      "source": [
        "print(f'User similarity: \\n{user_similarity}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8pVI-y47RS1"
      },
      "outputs": [],
      "source": [
        "print(f'Item similarity: \\n{item_similarity}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRStngsz7RS1"
      },
      "source": [
        "#### Selecting best users/items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldvUVsrN7RS2"
      },
      "outputs": [],
      "source": [
        "def pca_on_similarity(similarity: np.ndarray, k: int) -> np.ndarray:\n",
        "    pca: PCA = PCA(n_components=k)\n",
        "    pca.fit(similarity)\n",
        "    return pca.transform(similarity)\n",
        "\n",
        "def pca_embeddings_from_similarity(similarity: np.ndarray, bias: np.ndarray, k: int) -> np.ndarray:\n",
        "    return np.hstack([pca_on_similarity(similarity, k-1), bias[:,np.newaxis]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM9MUy817RS2"
      },
      "outputs": [],
      "source": [
        "user_embeddings: np.ndarray\n",
        "item_embeddings: np.ndarray\n",
        "if FORCE_NEW_EMBEDDINGS or not (os.path.isfile(USER_EMBEDDINGS_FILE) or os.path.isfile(ITEM_EMBEDDINGS_FILE)):\n",
        "    user_embeddings: np.ndarray = pca_embeddings_from_similarity(user_similarity, user_bias, USER_EMBEDDINGS_SIZE)\n",
        "    item_embeddings: np.ndarray = pca_embeddings_from_similarity(item_similarity, item_bias, ITEM_EMBEDDINGS_SIZE)\n",
        "\n",
        "    if SAVE_EMBEDDINGS:\n",
        "        with open(USER_EMBEDDINGS_FILE, 'wb') as f:\n",
        "            np.save(f, user_embeddings)\n",
        "        with open(ITEM_EMBEDDINGS_FILE, 'wb') as f:\n",
        "            np.save(f, item_embeddings)\n",
        "\n",
        "else:\n",
        "    with open(USER_EMBEDDINGS_FILE, 'rb') as f:\n",
        "        user_embeddings = np.load(f)\n",
        "    with open(ITEM_EMBEDDINGS_FILE, 'rb') as f:\n",
        "        item_embeddings = np.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpZFHBTZ7RS2"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9XHB5787RS2"
      },
      "source": [
        "#### Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnFhdpk87RS2"
      },
      "outputs": [],
      "source": [
        "def create_tensor_dataset(ratings: pd.DataFrame, user_embeddings: np.ndarray, item_embeddings: np.ndarray) -> TensorDataset:\n",
        "    users: np.ndarray = ratings['userId'].to_numpy()\n",
        "    items: np.ndarray = ratings['itemId'].to_numpy()\n",
        "\n",
        "    input_user_embeddings: np.ndarray = user_embeddings[users,:]\n",
        "    input_item_embeddings: np.ndarray = item_embeddings[items,:]\n",
        "\n",
        "    output_ratings: np.ndarray = ratings['rating'].to_numpy()\n",
        "\n",
        "    input_user_emb_tensor: torch.Tensor = torch.from_numpy(input_user_embeddings).to(dtype=torch.float32)\n",
        "    input_item_emb_tensor: torch.Tensor = torch.from_numpy(input_item_embeddings).to(dtype=torch.float32)\n",
        "    output_ratings_tensor: torch.Tensor = torch.from_numpy(output_ratings).to(dtype=torch.float32)\n",
        "\n",
        "    return TensorDataset(input_user_emb_tensor, input_item_emb_tensor, output_ratings_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVng745O7RS2"
      },
      "outputs": [],
      "source": [
        "train_loader: DataLoader = DataLoader(\n",
        "    create_tensor_dataset(train_ratings, user_embeddings, item_embeddings),\n",
        "    shuffle = True, batch_size = TRAIN_BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_loader: DataLoader = DataLoader(\n",
        "    create_tensor_dataset(test_ratings, user_embeddings, item_embeddings),\n",
        "    shuffle = False, batch_size = TEST_BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK2gZltY7RS2"
      },
      "source": [
        "#### Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVDzBZKq7RS2"
      },
      "outputs": [],
      "source": [
        "class Submetric:\n",
        "    def __init__(self, submetric_function: Callable, *metrics_used: str):\n",
        "        self.compute: Callable = submetric_function\n",
        "        self.metrics_used: list[str] = list(metrics_used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1hluDVp7RS2"
      },
      "outputs": [],
      "source": [
        "class MetricsHandler:\n",
        "    def __init__(self):\n",
        "        self.metrics: dict[str, torcheval.metrics.Metric[torch.Tensor]] = {}\n",
        "        self.submetrics: dict[str, Submetric] = {}\n",
        "\n",
        "    def add_metric(self, metric_name: str, metric: torcheval.metrics.Metric[torch.Tensor]) -> None:\n",
        "        self.metrics[metric_name] = metric\n",
        "\n",
        "    def add_submetric(self, submetric_name: str, submetric_function: Callable, *metrics_used: str) -> None:\n",
        "        self.submetrics[submetric_name] = Submetric(submetric_function, *metrics_used)\n",
        "\n",
        "    def update(self, input: torch.Tensor, target: torch.Tensor) -> None:\n",
        "        for metric in self.metrics.values():\n",
        "            metric.update(input, target)\n",
        "\n",
        "    def compute(self) -> dict[str, float]:\n",
        "        computed_metrics: dict[str, float] = {metric_name: metric.compute().item() for metric_name, metric in self.metrics.items()}\n",
        "        computed_submetrics: dict[str, float] = {submetric_name: submetric.compute(*[computed_metrics[metric_used] for metric_used in submetric.metrics_used]) for submetric_name, submetric in self.submetrics.items()}\n",
        "        computed_metrics.update(computed_submetrics)\n",
        "        return computed_metrics\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        for metric in self.metrics.values():\n",
        "            metric.reset()\n",
        "\n",
        "    def used_metrics(self, include_submetrics: bool = True) -> list[str]:\n",
        "        used_metrics = list(self.metrics.keys())\n",
        "        if include_submetrics:\n",
        "            used_metrics += list(self.submetrics.keys())\n",
        "\n",
        "        return used_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEXRGL3J7RS3"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: torch.nn.Module, loader: DataLoader, metrics: MetricsHandler) -> dict[str, float]:\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        user_emb: torch.Tensor\n",
        "        item_emb: torch.Tensor\n",
        "        expected_ratings: torch.Tensor\n",
        "        for user_emb, item_emb, expected_ratings in loader:\n",
        "            user_emb = user_emb.to(device=PYTORCH_DEVICE)\n",
        "            item_emb = item_emb.to(device=PYTORCH_DEVICE)\n",
        "            target = expected_ratings.to(device=PYTORCH_DEVICE)\n",
        "\n",
        "            outputs: torch.Tensor = model(user_emb, item_emb)\n",
        "\n",
        "            # Metrics are measured without the normalization\n",
        "            metrics.update(outputs*5, target*5)\n",
        "\n",
        "    computed_metrics: dict[str, float] = metrics.compute()\n",
        "\n",
        "    metrics.reset()\n",
        "\n",
        "    return computed_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KmEZlmP7RS3"
      },
      "outputs": [],
      "source": [
        "metrics: MetricsHandler = MetricsHandler()\n",
        "metrics.add_metric('mse', torcheval.metrics.regression.MeanSquaredError(device = PYTORCH_DEVICE))\n",
        "metrics.add_metric('r2',  torcheval.metrics.regression.R2Score(device = PYTORCH_DEVICE))\n",
        "metrics.add_submetric('rmse', lambda mse: np.sqrt(mse), 'mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ranking Evaluation"
      ],
      "metadata": {
        "id": "pMVuDXj0joab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ranking_precision(recommendations: pd.DataFrame, relevant_items: pd.DataFrame) -> float:\n",
        "    df: pd.DataFrame = recommendations.copy()\n",
        "    relevant_items = relevant_items.copy()\n",
        "    relevant_items['is_relevant'] = 1\n",
        "\n",
        "    df = df.merge(relevant_items, on=['userId', 'itemId'], how='left')\n",
        "    df['is_relevant'] = df['is_relevant'].fillna(0)\n",
        "    df['is_relevant'] = df['is_relevant'].astype(int)\n",
        "\n",
        "    return df['is_relevant'].mean()\n",
        "\n",
        "def ranking_recall(recommendations: pd.DataFrame, relevant_items: pd.DataFrame) -> float:\n",
        "    df: pd.DataFrame = recommendations.copy()\n",
        "    df['was_recommended'] = 1\n",
        "\n",
        "    df = df.merge(relevant_items[relevant_items['userId'].isin(df['userId'].unique())], on=['userId', 'itemId'], how='right')\n",
        "    df['was_recommended'] = df['was_recommended'].fillna(0)\n",
        "    df['was_recommended'] = df['was_recommended'].astype(int)\n",
        "\n",
        "    return df['was_recommended'].mean()\n",
        "\n",
        "def ranking_map(recommendations: pd.DataFrame, relevant_items: pd.DataFrame) -> float:\n",
        "    df: pd.DataFrame = recommendations.copy()\n",
        "    relevant_items = relevant_items.copy()\n",
        "    relevant_items['is_relevant'] = 1\n",
        "\n",
        "    df = df.merge(relevant_items, on=['userId', 'itemId'], how='left')\n",
        "    df['is_relevant'] = df['is_relevant'].fillna(0)\n",
        "    df['is_relevant'] = df['is_relevant'].astype(int)\n",
        "\n",
        "    n_users: int = len(df['userId'].unique())\n",
        "\n",
        "    df['numerator'] = df.groupby('userId')['is_relevant'].cumsum()\n",
        "    df['score'] = df['numerator'] / df['rank']\n",
        "\n",
        "    df = df[df['is_relevant'] != 0]\n",
        "    return df.groupby('userId')['score'].mean().sum() / n_users\n",
        "\n",
        "ranking_metrics: dict[str, Callable] = {\"Precision\": ranking_precision, \"Recall\": ranking_recall, \"MAP\": ranking_map}"
      ],
      "metadata": {
        "id": "ODnRs_xfjqzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ranking(recommendations: pd.DataFrame, relevant_items: pd.DataFrame, k: int | list[int], metrics: dict[str, Callable]) -> dict[str, float]:\n",
        "    if isinstance(k, int):\n",
        "        df: pd.DataFrame = recommendations.copy()\n",
        "        df = df[df['rank'] <= k]\n",
        "\n",
        "        relevant_items = relevant_items[['userId', 'itemId']].copy()\n",
        "\n",
        "        return {k: metric(df, relevant_items) for k, metric in metrics.items()}\n",
        "\n",
        "    else:\n",
        "        return {f'{key}@{top_k}': value for top_k in k for key, value in evaluate_ranking(recommendations, relevant_items, top_k, metrics).items()}"
      ],
      "metadata": {
        "id": "By79TMI2js_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKwU9d-p7RS3"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tohz56Md7RS3"
      },
      "outputs": [],
      "source": [
        "class UserTower(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        user_embeddings_size: int,\n",
        "        output_size: int\n",
        "    ):\n",
        "        super(UserTower, self).__init__()\n",
        "\n",
        "        self.l1 = torch.nn.Linear(user_embeddings_size, 2048, dtype=torch.float32)\n",
        "        self.drop1 =  torch.nn.Dropout(DROPOUT)\n",
        "        self.l2 = torch.nn.Linear(2048, 2048, dtype=torch.float32)\n",
        "        self.drop2 =  torch.nn.Dropout(DROPOUT)\n",
        "        self.output_l = torch.nn.Linear(2048, output_size, dtype=torch.float32)\n",
        "\n",
        "    def forward(self, user_embeddings):\n",
        "        temp = self.drop1(torch.relu(self.l1(user_embeddings)))\n",
        "        temp = self.drop2(torch.relu(self.l2(temp)))\n",
        "        return torch.relu(self.output_l(temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH5oqngR7RS7"
      },
      "outputs": [],
      "source": [
        "class ItemTower(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        item_embeddings_size: int,\n",
        "        output_size: int\n",
        "    ):\n",
        "        super(ItemTower, self).__init__()\n",
        "\n",
        "        self.l1 = torch.nn.Linear(item_embeddings_size, 2048, dtype=torch.float32)\n",
        "        self.drop1 =  torch.nn.Dropout(DROPOUT)\n",
        "        self.l2 = torch.nn.Linear(2048, 2048, dtype=torch.float32)\n",
        "        self.drop2 =  torch.nn.Dropout(DROPOUT)\n",
        "        self.output_l = torch.nn.Linear(2048, output_size, dtype=torch.float32)\n",
        "\n",
        "    def forward(self, item_embeddings):\n",
        "        temp = self.drop1(torch.relu(self.l1(item_embeddings)))\n",
        "        temp = self.drop2(torch.relu(self.l2(temp)))\n",
        "        return torch.relu(self.output_l(temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGGq0Ysb7RS8"
      },
      "outputs": [],
      "source": [
        "class TwoTowerModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        user_embeddings_size: int,\n",
        "        item_embeddings_size: int,\n",
        "        towers_final_size: int,\n",
        "    ):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "\n",
        "        self.user_tower: UserTower = UserTower(user_embeddings_size, towers_final_size)\n",
        "        self.item_tower: ItemTower = ItemTower(item_embeddings_size, towers_final_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        user_embeddings,\n",
        "        item_embeddings\n",
        "    ):\n",
        "        user_temp: torch.Tensor = self.user_tower(user_embeddings)\n",
        "        item_temp: torch.Tensor = self.item_tower(item_embeddings)\n",
        "\n",
        "        return torch.sum(user_temp * item_temp, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQnlFbep7RS8"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u85cpEdE7RS8"
      },
      "outputs": [],
      "source": [
        "model: torch.nn.Module\n",
        "loss_fn: torch.nn.MSELoss = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "if FORCE_TRAINNING or not os.path.isfile(MODEL_FILE):\n",
        "    model = TwoTowerModel(\n",
        "        user_embeddings_size = USER_EMBEDDINGS_SIZE,\n",
        "        item_embeddings_size = ITEM_EMBEDDINGS_SIZE,\n",
        "        towers_final_size =    TOWERS_FINAL_SIZE\n",
        "    )\n",
        "    model.to(device=PYTORCH_DEVICE)\n",
        "\n",
        "    optimizer: torch.optim.Adam = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "    try:\n",
        "        for epoch in (tqdm(range(1, N_EPOCHS+1), desc='Training', unit='epoch') if USE_TQDM else range(1, N_EPOCHS+1)):\n",
        "            if VERBOSE_TRAINNING or not USE_TQDM:\n",
        "                print(f'current epoch: {epoch} / {N_EPOCHS}', end=('\\n' if VERBOSE_TRAINNING else '\\r'))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            user_emb: torch.Tensor\n",
        "            item_emb: torch.Tensor\n",
        "            expected_ratings: torch.Tensor\n",
        "            for user_emb, item_emb, expected_ratings in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                user_emb = user_emb.to(device=PYTORCH_DEVICE)\n",
        "                item_emb = item_emb.to(device=PYTORCH_DEVICE)\n",
        "                expected_ratings = expected_ratings.to(device=PYTORCH_DEVICE)\n",
        "\n",
        "                outputs: torch.Tensor = model(user_emb, item_emb)\n",
        "                loss: torch.Tensor = torch.sqrt(loss_fn(outputs, expected_ratings))\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += np.square(loss.item()) * len(user_emb)\n",
        "\n",
        "            if VERBOSE_TRAINNING:\n",
        "                # RMSE assuming normalized input\n",
        "                tqdm.write(f'epoch loss: {np.sqrt((epoch_loss * 25) / len(train_loader.dataset))}')\n",
        "                tqdm.write(f'test loss: {evaluate(model, test_loader, metrics)[\"rmse\"]}\\n')\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "    if not (VERBOSE_TRAINNING or USE_TQDM):\n",
        "        print('')\n",
        "\n",
        "    if SAVE_MODEL:\n",
        "        torch.save(model, MODEL_FILE)\n",
        "\n",
        "else:\n",
        "    model = torch.load(MODEL_FILE)\n",
        "    model.to(device = PYTORCH_DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVLhMwuo7RS8"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVZIQJ8a7RS8"
      },
      "outputs": [],
      "source": [
        "PRECISION: int = 5\n",
        "MAX_METRIC_NAME_LENGTH: int = len(max(metrics.used_metrics(), key=len))\n",
        "\n",
        "computed_train_eval: dict[str, float] = evaluate(model, train_loader, metrics)\n",
        "print('Train Eval:')\n",
        "print('\\n'.join(f'{metric_name:<{MAX_METRIC_NAME_LENGTH}} : {metric_eval:.{PRECISION}f}' for metric_name, metric_eval in computed_train_eval.items()))\n",
        "\n",
        "computed_test_eval: dict[str, float] = evaluate(model, test_loader, metrics)\n",
        "print('\\nTest Eval:')\n",
        "print('\\n'.join(f'{metric_name:<{MAX_METRIC_NAME_LENGTH}} : {metric_eval:.{PRECISION}f}' for metric_name, metric_eval in computed_test_eval.items()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model: torch.nn.Module, df: pd.DataFrame, user_embeddings: np.ndarray, item_embeddings: np.ndarray) -> pd.DataFrame:\n",
        "    predictions: pd.DataFrame = pd.DataFrame({\"userId\": [], \"itemId\": [], \"prediction\": []})\n",
        "    predictions['userId'] = predictions['userId'].astype(int)\n",
        "    predictions['itemId'] = predictions['itemId'].astype(int)\n",
        "\n",
        "    items_set: set[int] = set(df['itemId'])\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user in tqdm(df['userId'].unique()):\n",
        "            user_df: pd.DataFrame = df[df['userId'] == user]\n",
        "            new_items_ids: list[int] = list(items_set - set(user_df['itemId']))\n",
        "\n",
        "            input_user_embeddings: torch.Tensor = torch.from_numpy(np.repeat(user_embeddings[user][np.newaxis, :], len(new_items_ids), axis=0)).to(device=PYTORCH_DEVICE, dtype=torch.float32)\n",
        "            input_item_embeddings: torch.Tensor = torch.from_numpy(item_embeddings[new_items_ids,:]).to(device=PYTORCH_DEVICE, dtype=torch.float32)\n",
        "\n",
        "            user_predictions: list[float] = model(input_user_embeddings, input_item_embeddings).squeeze().tolist()\n",
        "\n",
        "            predictions = pd.concat([predictions, pd.DataFrame({\"userId\": user, \"itemId\": new_items_ids, \"prediction\": user_predictions})])\n",
        "\n",
        "    predictions['rank'] = predictions.groupby(\"userId\")['prediction'].rank('first', ascending=False)\n",
        "    predictions['rank'] = predictions['rank'].astype(int)\n",
        "    predictions = predictions.sort_values([\"userId\", \"rank\"])\n",
        "    return predictions\n",
        "\n",
        "ranked_predictions: pd.DataFrame = get_predictions(model, train_ratings, user_embeddings, item_embeddings)\n",
        "display(ranked_predictions.head())"
      ],
      "metadata": {
        "id": "2FS6DHH3j0k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_ranking(ranked_predictions, test_ratings, k = [5, 10, 20], metrics = ranking_metrics)"
      ],
      "metadata": {
        "id": "jgj2iuDtj7ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User KNN for comparison"
      ],
      "metadata": {
        "id": "jHtB5LCOvJGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATINGS_FILE: str = f'train.csv'\n",
        "TEST_RATINGS_FILE: str = f'test.csv'\n",
        "USERKNN_OUT_FILE: str = f'{WORKING_DIR}/userknn_out.csv'"
      ],
      "metadata": {
        "id": "yp-O4qfHwNwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN for rating prediction"
      ],
      "metadata": {
        "id": "aAG7URxgUAb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Going back to the original ratings\n",
        "train_ratings['rating'] *= 5\n",
        "test_ratings['rating'] *= 5\n",
        "train_ratings.to_csv(TRAIN_RATINGS_FILE, index=False, header=False, sep='\\t')\n",
        "test_ratings.to_csv(TEST_RATINGS_FILE, index=False, header=False, sep='\\t')\n",
        "train_ratings['rating'] /= 5\n",
        "test_ratings['rating'] /= 5\n",
        "\n",
        "UserKNN_ratings(TRAIN_RATINGS_FILE, TEST_RATINGS_FILE).compute()"
      ],
      "metadata": {
        "id": "qlpLni3NS55R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN for item recommendation"
      ],
      "metadata": {
        "id": "isSkdRJFUGG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratings.to_csv(TRAIN_RATINGS_FILE, index=False, header=False, sep='\\t')\n",
        "test_ratings.to_csv(TEST_RATINGS_FILE, index=False, header=False, sep='\\t')\n",
        "if not os.path.isfile(USERKNN_OUT_FILE):\n",
        "    UserKNN(TRAIN_RATINGS_FILE, TEST_RATINGS_FILE, output_file=USERKNN_OUT_FILE, as_similar_first=False, rank_length=20).compute() # Creates the predictions for the any pair that didn't appear on the train data"
      ],
      "metadata": {
        "id": "fI-gPtP51dR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userknn_predictions: pd.DataFrame = pd.read_csv(USERKNN_OUT_FILE, names=['userId', 'itemId', 'prediction'], sep='\\t', header=0)\n",
        "\n",
        "userknn_predictions['rank'] = userknn_predictions.groupby(\"userId\")['prediction'].rank('first', ascending=False)\n",
        "userknn_predictions['rank'] = userknn_predictions['rank'].astype(int)\n",
        "userknn_predictions = userknn_predictions.sort_values([\"userId\", \"rank\"])\n",
        "\n",
        "display(userknn_predictions.head())"
      ],
      "metadata": {
        "id": "o7L17tLExwGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(userknn_predictions)"
      ],
      "metadata": {
        "id": "TDsTLr45E2aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_ranking(userknn_predictions, test_ratings, k = [5, 10, 20], metrics = ranking_metrics)"
      ],
      "metadata": {
        "id": "iJ72jt2B2djA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison"
      ],
      "metadata": {
        "id": "_D4HhqjCMI6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation considering only items that each user didn't dislike as being relevant\n",
        "ranking_eval = evaluate_ranking(ranked_predictions, test_ratings[test_ratings['rating'] >= 0.6], k = [5, 10, 20], metrics = ranking_metrics)\n",
        "knn_eval = evaluate_ranking(userknn_predictions, test_ratings[test_ratings['rating'] >= 0.6], k = [5, 10, 20], metrics = ranking_metrics)"
      ],
      "metadata": {
        "id": "IE6nPcGeHw1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(pd.DataFrame({'model': ranking_eval, 'knn': knn_eval}))"
      ],
      "metadata": {
        "id": "Suwm_Z2uIFD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "r9XHB5787RS2",
        "sK2gZltY7RS2"
      ],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}