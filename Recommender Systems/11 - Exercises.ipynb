{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3047da87",
   "metadata": {},
   "source": [
    "# Aula 12 - Multi-Armed Bandits - 10-armed testbed - Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02f309",
   "metadata": {},
   "source": [
    "Os exercícios dessa semana consistem em auxiliar na interpretação do código do 10-armed testbed framework, visto em sala, a fim de entender o funcionamento dos três algoritmos principais de MAB: Epsilon Greedy, UCB1 e Thompson Sampling.\n",
    "\n",
    "Estude o código da aula (Aula12_Exemplos.ipynb) procurando relacionar o código com os slides da aula. O código está separado por classes para facilitar o entendimento:\n",
    "\n",
    "- **Classe Testbed:**  Contém as ações possíveis (braços do MAB), suas distribuições e o braço considerado a ação ótima (maior recompensa). \n",
    "- **Classe Agent:** Classe pai para os demais algoritmos MAB. Possui algumas funções de uso comum, como reset(), interpreter() e exploit().\n",
    "- **Classe Random:** Algoritmo MAB que realiza a escolha aleatória de uma ação (somente explora). \n",
    "- **Classe ExploreOnce:** Algoritmo MAB que escolhe aleatoriamente cada ação possível uma única vez e depois somente explota.\n",
    "- **Classe Epsilon:** Algoritmo MAB que usa um fator epsilon para decidir entre explorar e explotar.\n",
    "- **Classe ExploreSteps:** Algoritmo MAB que realiza um número fixo de jogadas na fase exploração e depois somente explota.\n",
    "- **Classe UCB1:** Implementação do algoritmo UCB1.\n",
    "- **Classe Thompson:** Implementação do algoritmo Thompson Sampling.\n",
    "- **Classe Environment:** Classe que contém o loop principal de iterações, jogadas e controla o Testbed e os algoritmos MAB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478fee7",
   "metadata": {},
   "source": [
    "**Exercício 01:** Explique o que representa os valores contidos na variável actArr da classe Testbed. Utilize os slides da aula para auxiliar na sua resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c0c1f",
   "metadata": {},
   "source": [
    "Resposta: Os valores da variável `actArr` representa a recompensa real esperada ($q_{*}(a)$) para cada braço na configuração do Multi-Armed Bandit. Eles são gerados aleatoriamente usando uma distribuição normal com a média e o desvio padrão fornecidos. Cada braço tem seu próprio valor de recompensa, e o braço ótimo é aquele com a maior recompensa, armazenado em `optim`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52b163",
   "metadata": {},
   "source": [
    "**Exercício 02:** Explique o funcionamento da função action() da classe Epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d77d3",
   "metadata": {},
   "source": [
    "Resposta: A função `action()` da classe `Epsilon` é responsável por determinar se o agente explorará novas ações (explore) ou explorará informações conhecidas para maximizar recompensas (exploit). A decisão é baseada em uma probabilidade aleatória (`randProb`) que é gerada entre 0 e 1. Se esse valor aleatório for menor que `eProb` (probabilidade epsilon), o agente explora selecionando aleatoriamente uma ação entre as opções disponíveis. Isso permite que o agente experimente novas ações que ele ainda não avaliou completamente.\n",
    "\n",
    "Por outro lado, se a probabilidade aleatória for maior ou igual a `eProb`, o agente explora selecionando a ação com a maior recompensa estimada usando a função `exploit()`. Isso ajuda o agente a maximizar sua recompensa com base no conhecimento que ele já reuniu. A ação escolhida, seja por exploração ou exploração, é armazenada em `lastAction` e então retornada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca52b76",
   "metadata": {},
   "source": [
    "**Exercício 03:** Explique o funcionamento das funções action() e interpreter() da classe Thompson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25506e63",
   "metadata": {},
   "source": [
    "Resposta: Na classe `Thompson`, as funções `action()` e `interpreter()` trabalham juntas para implementar o Thompson Sampling, que é uma abordagem probabilística para resolver o trade-off exploração-exploração em problemas de Multi-Armed Bandits.\n",
    "\n",
    "A função `action()` usa uma distribuição Beta para decidir qual braço (ou ação) selecionar. Para cada braço, uma amostra é extraída de uma distribuição Beta com parâmetros baseados nas contagens de sucesso e falha para aquele braço. Quanto mais bem-sucedido um braço for, maior a probabilidade de ter um valor de amostra maior, mas ainda há uma chance de exploração devido à aleatoriedade no processo de amostragem. Após gerar uma amostra para cada braço, a função seleciona o braço com o maior valor amostrado e o retorna como a ação escolhida.\n",
    "\n",
    "A função `interpreter()` atualiza o conhecimento do agente com base na recompensa recebida da última ação. Se a recompensa for positiva, a contagem de sucesso para aquele braço é aumentada e, se a recompensa for negativa, a contagem de falha é atualizada. Esse ajuste de sucessos e falhas ajusta os parâmetros da distribuição Beta, o que influencia futuras tomadas de decisão. A função também aumenta o passo de tempo, rastreando o progresso do agente ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac46c04",
   "metadata": {},
   "source": [
    "**Exercício 04:** Explique resumidamente o funcionamento da função play() da classe Environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80de16d",
   "metadata": {},
   "source": [
    "Resposta: A função `play()` da classe `Environment` é responsável por executar o loop principal de iterações, jogadas e controlar o Testbed e os algoritmos MAB. Ela inicializa o ambiente, define os parâmetros de simulação, executa as iterações e coleta os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23076567",
   "metadata": {},
   "source": [
    "**Exercício 05:** Dado o último gráfico do exemplo, que contém o desempenho dos algoritmos UCB1, Thompson, Epsilon e EpsilonSteps, explique por que depois de um número de jogadas, o Epsilon Steps mantém um valor de % de ação ótima constante. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b169f",
   "metadata": {},
   "source": [
    "Resposta: O EpsilonSteps mantém um valor constante porque ele opera em duas fases: primeiro explora por N passos fixos, depois apenas explota (usa) o conhecimento adquirido. Como não há mais exploração após os N passos iniciais, a taxa de seleção da ação ótima se estabiliza em um valor constante, que depende apenas do conhecimento obtido durante a fase inicial de exploração. Diferente dos outros algoritmos (UCB1, Thompson, Epsilon) que continuam explorando e aprendendo ao longo do tempo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
