{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58dbcadb",
   "metadata": {},
   "source": [
    "## Aula 06 - Filtragem Híbrida - Exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc39da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pow, sqrt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pickle\n",
    "from caserec.recommenders.rating_prediction.item_attribute_knn import ItemAttributeKNN\n",
    "from caserec.recommenders.item_recommendation.bprmf import BprMF\n",
    "from caserec.recommenders.item_recommendation.itemknn import ItemKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af07eb8",
   "metadata": {},
   "source": [
    "### Importar base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2652d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under ml-20m-compact.tar (32).gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x dataset/\n",
      "x dataset/tags_sample.csv\n",
      "x dataset/._.DS_Store\n",
      "x dataset/.DS_Store\n",
      "x dataset/movies_sample.csv\n",
      "x dataset/._genome-tags.csv\n",
      "x dataset/genome-tags.csv\n",
      "x dataset/._ml-youtube.csv\n",
      "x dataset/ml-youtube.csv\n",
      "x dataset/._genome-scores.csv\n",
      "x dataset/genome-scores.csv\n",
      "x dataset/ratings_sample.csv\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "!python3 -m wget https://github.com/mmanzato/MBABigData/raw/master/ml-20m-compact.tar.gz\n",
    "!tar -xvzf ml-20m-compact.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283e7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under ml-20m-features.tar (18).gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x features/\n",
      "x features/._m4infus_max_histogram_300_sn.arq\n",
      "x features/m4infus_max_histogram_300_sn.arq\n",
      "x features/._mm_avg_histogram_100_sn.arq\n",
      "x features/mm_avg_histogram_100_sn.arq\n",
      "x features/._visual_histogram_100_sn.arq\n",
      "x features/visual_histogram_100_sn.arq\n",
      "x features/._visual_histogram_50_sn.arq\n",
      "x features/visual_histogram_50_sn.arq\n",
      "x features/._aural_histogram_50.arq\n",
      "x features/aural_histogram_50.arq\n",
      "x features/._mm_max_histogram_300.arq\n",
      "x features/mm_max_histogram_300.arq\n",
      "x features/._m4infus_max_histogram_50.arq\n",
      "x features/m4infus_max_histogram_50.arq\n",
      "x features/._mm_max_histogram_100.arq\n",
      "x features/mm_max_histogram_100.arq\n",
      "x features/._mm_max_histogram_50_sn.arq\n",
      "x features/mm_max_histogram_50_sn.arq\n",
      "x features/._visual_histogram_100.arq\n",
      "x features/visual_histogram_100.arq\n",
      "x features/._visual_histogram_300.arq\n",
      "x features/visual_histogram_300.arq\n",
      "x features/._aural_histogram_100_sn.arq\n",
      "x features/aural_histogram_100_sn.arq\n",
      "x features/._mm_avg_histogram_100.arq\n",
      "x features/mm_avg_histogram_100.arq\n",
      "x features/._mm_max_histogram_100_sn.arq\n",
      "x features/mm_max_histogram_100_sn.arq\n",
      "x features/._mm_sum_histogram_100_sn.arq\n",
      "x features/mm_sum_histogram_100_sn.arq\n",
      "x features/._mm_avg_histogram_300.arq\n",
      "x features/mm_avg_histogram_300.arq\n",
      "x features/._visual_histogram_300_sn.arq\n",
      "x features/visual_histogram_300_sn.arq\n",
      "x features/._mm_avg_histogram_300_sn.arq\n",
      "x features/mm_avg_histogram_300_sn.arq\n",
      "x features/._m4infus_max_histogram_100.arq\n",
      "x features/m4infus_max_histogram_100.arq\n",
      "x features/._m4infus_max_histogram_100_sn.arq\n",
      "x features/m4infus_max_histogram_100_sn.arq\n",
      "x features/._mm_sum_histogram_50_sn.arq\n",
      "x features/mm_sum_histogram_50_sn.arq\n",
      "x features/._m4infus_max_histogram_300.arq\n",
      "x features/m4infus_max_histogram_300.arq\n",
      "x features/._mm_avg_histogram_50_sn.arq\n",
      "x features/mm_avg_histogram_50_sn.arq\n",
      "x features/._mm_sum_histogram_50.arq\n",
      "x features/mm_sum_histogram_50.arq\n",
      "x features/._visual_histogram_50.arq\n",
      "x features/visual_histogram_50.arq\n",
      "x features/._aural_histogram_50_sn.arq\n",
      "x features/aural_histogram_50_sn.arq\n",
      "x features/._mm_sum_histogram_300.arq\n",
      "x features/mm_sum_histogram_300.arq\n",
      "x features/._m4infus_max_histogram_50_sn.arq\n",
      "x features/m4infus_max_histogram_50_sn.arq\n",
      "x features/._mm_max_histogram_50.arq\n",
      "x features/mm_max_histogram_50.arq\n",
      "x features/._mm_avg_histogram_50.arq\n",
      "x features/mm_avg_histogram_50.arq\n",
      "x features/._mm_max_histogram_300_sn.arq\n",
      "x features/mm_max_histogram_300_sn.arq\n",
      "x features/._mm_sum_histogram_300_sn.arq\n",
      "x features/mm_sum_histogram_300_sn.arq\n",
      "x features/._mm_sum_histogram_100.arq\n",
      "x features/mm_sum_histogram_100.arq\n",
      "x features/._aural_histogram_300.arq\n",
      "x features/aural_histogram_300.arq\n",
      "x features/._aural_histogram_300_sn.arq\n",
      "x features/aural_histogram_300_sn.arq\n",
      "x features/._aural_histogram_100.arq\n",
      "x features/aural_histogram_100.arq\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wget https://github.com/mmanzato/MBABigData/raw/master/ml-20m-features.tar.gz\n",
    "! tar -xvzf ml-20m-features.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a8961",
   "metadata": {},
   "source": [
    "***Exercício 01:*** Implemente uma hibridização monolítica/combinação usando a seguinte heurística:\n",
    "- Uso do algoritmo ItemAtributeKNN, sendo a hibridização feita no cálculo das similaridades entre os itens.\n",
    "- Se a quantidade de usuários que avaliaram ambos os itens for maior que um limiar L1, calcule a similaridade entre esses itens usando cosseno aplicado à representação baseada em notas.\n",
    "- Caso contrário, calcule a similaridade entre os itens usando tags, características visuais e características aurais. Pondere cada uma das modalidades via pesos passados por parâmetro. \n",
    "\n",
    "Compare os resultados do algoritmo híbrido com as versões isoladas do mesmo algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f89d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the csv files\n",
    "movies = pd.read_csv('./dataset/movies_sample.csv')\n",
    "ratings = pd.read_csv('./dataset/ratings_sample.csv')\n",
    "tags = pd.read_csv('./dataset/tags_sample.csv')\n",
    "\n",
    "# Merge movies and ratings df\n",
    "df = ratings[['userId', 'movieId', 'rating']]\n",
    "df = df.merge(movies[['movieId', 'title']])\n",
    "\n",
    "# Map the userId and movieId \n",
    "map_users = {user: idx for idx, user in enumerate(df.userId.unique())}\n",
    "map_items = {item: idx for idx, item in enumerate(df.movieId.unique())}\n",
    "df['userId'] = df['userId'].map(map_users)\n",
    "df['movieId'] = df['movieId'].map(map_items)\n",
    "tags['userId'] = tags['userId'].map(map_users)\n",
    "tags['movieId'] = tags['movieId'].map(map_items)\n",
    "\n",
    "# Split in train and test\n",
    "train, test = train_test_split(df, test_size=0.20, random_state=2)\n",
    "train.to_csv('train.dat', index=False, header=False, sep='\\t')\n",
    "test.to_csv('test.dat', index=False, header=False, sep='\\t')\n",
    "\n",
    "map_title = {}\n",
    "for _, row in df.iterrows():\n",
    "    map_title[row.movieId] = row.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77fe386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions that will be used ahead\n",
    "\n",
    "# Get the tags of the movie\n",
    "def get_tags(movieId):\n",
    "    if movieId not in tags['movieId'].values:\n",
    "        return []\n",
    "    return tags.loc[(tags.movieId==movieId),'tag'].tolist()\n",
    "\n",
    "# Get the common users that rated both movies\n",
    "def get_common_raters_between_movies(movieId1, movieId2):\n",
    "    df_movie1 = df[df['movieId'] == movieId1][['userId']]\n",
    "    df_movie2 = df[df['movieId'] == movieId2][['userId']]\n",
    "    \n",
    "    merged_users = pd.merge(df_movie1, df_movie2, on='userId')\n",
    "    return merged_users['userId'].tolist()\n",
    "\n",
    "# Calculate the Jaccard similarity by tags between 2 movies \n",
    "def item_sim_jaccard_tag(movieId1, movieId2):\n",
    "    tag_list1 = get_tags(movieId1)\n",
    "    tag_list2 = get_tags(movieId2)\n",
    "    common_tags = list(set(tag_list1) & set(tag_list2))\n",
    "    if len(common_tags) == 0:\n",
    "        return 0\n",
    "    return len(common_tags) / len(set(tag_list1 + tag_list2))\n",
    "\n",
    "# Generate the similarity files\n",
    "def generate_dat_file_sim(sim_df, filename):\n",
    "    with open(filename + \".dat\", 'w') as arq_sim_matrix:\n",
    "        for i in range(len(sim_df)):\n",
    "            for j in range(len(sim_df)):\n",
    "                if i < j:\n",
    "                    arq_sim_matrix.write(str(i) + '\\t' + str(j) + '\\t' + str(sim_df[i][j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7f279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating-based cosine similarity calculation\n",
    "\n",
    "n_users = train['userId'].max()\n",
    "n_items = train['movieId'].max()\n",
    "\n",
    "A = np.zeros((n_users+1,n_items+1))\n",
    "for line in train.itertuples():\n",
    "    A[line[1],line[2]] = line[3]\n",
    "\n",
    "# Similarity matrix of all movies, based on ratings\n",
    "sim_matrix_rating = pairwise_distances(A.T, metric=\"cosine\")\n",
    "\n",
    "with open('sim_r_matrix.dat', 'w') as arq_sim_matrix:\n",
    "    for i in range(len(sim_matrix_rating)):\n",
    "        for j in range(len(sim_matrix_rating)):\n",
    "            if i < j:\n",
    "                arq_sim_matrix.write(str(i) + '\\t' + str(j) + '\\t' + str(sim_matrix_rating[i][j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6954b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags-based Jaccard similarity calculation\n",
    "\n",
    "n_items = train['movieId'].max()\n",
    "\n",
    "sim_matrix_tag = np.zeros((n_items+1,n_items+1))\n",
    "\n",
    "\n",
    "# Similarity matrix of all movies, based on tags\n",
    "sim_matrix_tag = pairwise_distances(A.T, metric=\"cosine\")\n",
    "\n",
    "with open('sim_matrix_tag.dat', 'w') as arq_sim_matrix:\n",
    "    for i in range(len(sim_matrix_tag)):\n",
    "        for j in range(len(sim_matrix_tag)):\n",
    "            if i < j:\n",
    "                sim_matrix_tag[i][j] = item_sim_jaccard_tag(i, j)\n",
    "                arq_sim_matrix.write(str(i) + '\\t' + str(j) + '\\t' + str(sim_matrix_tag[i][j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac9d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual_Histogram-based cosine similarity calculation\n",
    "\n",
    "with open('./features/visual_histogram_50.arq', 'rb') as arq_visualHistograms:\n",
    "    visualHistograms = pickle.load(arq_visualHistograms)\n",
    "\n",
    "# Similarity matrix of all movies, based on visual histograms\n",
    "sim_matrix_visual_hist = pairwise_distances(visualHistograms, metric=\"cosine\")\n",
    "\n",
    "with open('sim_matrix_visual_hist.dat', 'w') as arq_sim_matrix:\n",
    "    for i in range(len(sim_matrix_visual_hist)):\n",
    "        for j in range(len(sim_matrix_visual_hist)):\n",
    "            if i < j:\n",
    "                arq_sim_matrix.write(str(i) + '\\t' + str(j) + '\\t' + str(sim_matrix_visual_hist[i][j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc93fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aural_Histogram-based cosine similarity calculation\n",
    "\n",
    "with open('./features/aural_histogram_50_sn.arq', 'rb') as arq_auralHistograms:\n",
    "    auralHistograms = pickle.load(arq_auralHistograms)\n",
    "\n",
    "# Similarity matrix of all movies, based on aural histograms\n",
    "sim_matrix_aural_hist = pairwise_distances(auralHistograms, metric=\"cosine\")\n",
    "\n",
    "with open('sim_matrix_aural_hist.dat', 'w') as arq_sim_matrix:\n",
    "    for i in range(len(sim_matrix_aural_hist)):\n",
    "        for j in range(len(sim_matrix_aural_hist)):\n",
    "            if i < j:\n",
    "                arq_sim_matrix.write(str(i) + '\\t' + str(j) + '\\t' + str(sim_matrix_aural_hist[i][j]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd8f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid matrix using sim_matrix_tag, sim_matrix_visual_hist and sim_matrix_aural_hist\n",
    "def hybrid_sim(L1, w_tag, w_visual, w_aural):\n",
    "    sim_matrix_hybrid = sim_matrix_rating.copy()\n",
    "    \n",
    "    # Normalize the weights if their sum is not 1\n",
    "    w_total = w_tag + w_visual + w_aural\n",
    "    w_tag /= w_total\n",
    "    w_visual /= w_total\n",
    "    w_aural /= w_total\n",
    "\n",
    "    # Iterate through the upper triangle of the matrix (i < j)\n",
    "    for i in range(len(sim_matrix_hybrid)):\n",
    "        for j in range(i+1, len(sim_matrix_hybrid)):  # Start from i+1 to avoid checking i < j\n",
    "            # If the number of common raters is below the threshold L1\n",
    "            if len(get_common_raters_between_movies(i, j)) <= L1:\n",
    "                # Weighted sum of tag, visual, and aural similarities\n",
    "                sim_matrix_hybrid[i][j] = (\n",
    "                    w_tag * sim_matrix_tag[i, j] +\n",
    "                    w_visual * sim_matrix_visual_hist[i, j] +\n",
    "                    w_aural * sim_matrix_aural_hist[i, j]\n",
    "                )\n",
    "                # Maintain symmetry\n",
    "                sim_matrix_hybrid[j][i] = sim_matrix_hybrid[i][j]\n",
    "    \n",
    "    return sim_matrix_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd1a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Rating Prediction > Item Attribute KNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 4.682458 sec\n",
      "prediction_time:: 0.434747 sec\n",
      "Eval:: MAE: 0.664563 RMSE: 0.866486 \n"
     ]
    }
   ],
   "source": [
    "# Test 1 - Rating-Based Similarity Only\n",
    "# (L1, w_tag, w_visual, w_aural) = (0, 1/3, 1/3, 1/3)\n",
    "# Since L1 = 0, the hybrid similarity matrix will only use sim_matrix_rating, \n",
    "# ignoring the tag, visual, and aural similarity matrices. \n",
    "\n",
    "sim_hib_matrix = hybrid_sim(0, 1/3, 1/3, 1/3)\n",
    "generate_dat_file_sim(sim_hib_matrix, \"sim_hib_matrix\")\n",
    "ItemAttributeKNN('train.dat', 'test.dat', similarity_file='sim_hib_matrix.dat', as_similar_first=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2170f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Rating Prediction > Item Attribute KNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 4.532275 sec\n",
      "prediction_time:: 0.383004 sec\n",
      "Eval:: MAE: 0.69465 RMSE: 0.909092 \n"
     ]
    }
   ],
   "source": [
    "# Test 2 - Tag-Based Similarity Only \n",
    "# (L1 > 0, w_tag, w_visual, w_aural) = (5, 1, 0, 0)\n",
    "# This test will only use the tag-based similarity matrix (sim_matrix_tag) \n",
    "# by setting w_tag = 1, and w_visual = w_aural = 0. It uses tag-based similarity \n",
    "# only when there are 5 or fewer common raters (L1 = 5).\n",
    "\n",
    "sim_tag_matrix = hybrid_sim(5, 1, 0, 0)\n",
    "generate_dat_file_sim(sim_tag_matrix, \"sim_tag_matrix\")\n",
    "ItemAttributeKNN('train.dat', 'test.dat', similarity_file='sim_tag_matrix.dat', as_similar_first=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e853eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Rating Prediction > Item Attribute KNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 4.557132 sec\n",
      "prediction_time:: 0.260340 sec\n",
      "Eval:: MAE: 0.664174 RMSE: 0.865856 \n"
     ]
    }
   ],
   "source": [
    "# Test 3 - Visual-Based Similarity Only \n",
    "# (L1 > 0, w_tag, w_visual, w_aural) = (5, 0, 1, 0)\n",
    "# This test will only use the visual-based similarity matrix (sim_matrix_visual_hist) \n",
    "# by setting w_visual = 1, and w_tag = w_aural = 0. It uses visual-based similarity \n",
    "# only when there are 5 or fewer common raters (L1 = 5).\n",
    "\n",
    "sim_visual_matrix = hybrid_sim(5, 0, 1, 0)\n",
    "generate_dat_file_sim(sim_visual_matrix, \"sim_visual_matrix\")\n",
    "ItemAttributeKNN('train.dat', 'test.dat', similarity_file='sim_visual_matrix.dat', as_similar_first=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73276480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Rating Prediction > Item Attribute KNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 4.563418 sec\n",
      "prediction_time:: 0.447876 sec\n",
      "Eval:: MAE: 0.672189 RMSE: 0.877902 \n"
     ]
    }
   ],
   "source": [
    "# Test 4 - Aural-Based Similarity Only \n",
    "# (L1 > 0, w_tag, w_visual, w_aural) = (5, 0, 0, 1)\n",
    "# This test will only use the aural-based similarity matrix (sim_matrix_aural_hist) \n",
    "# by setting w_aural = 1, and w_tag = w_visual = 0. It uses aural-based similarity \n",
    "# only when there are 5 or fewer common raters (L1 = 5).\n",
    "\n",
    "sim_aural_matrix = hybrid_sim(5, 0, 0, 1)\n",
    "generate_dat_file_sim(sim_aural_matrix, \"sim_aural_matrix\")\n",
    "ItemAttributeKNN('train.dat', 'test.dat', similarity_file='sim_aural_matrix.dat', as_similar_first=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8881fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Rating Prediction > Item Attribute KNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 4.561417 sec\n",
      "prediction_time:: 0.304187 sec\n",
      "Eval:: MAE: 0.682064 RMSE: 0.891725 \n"
     ]
    }
   ],
   "source": [
    "# Test 5 - Hybrid Similarity with Equal Weights \n",
    "# (L1 > 0, w_tag, w_visual, w_aural) = (5, 1/3, 1/3, 1/3)\n",
    "# This test will use a hybrid similarity matrix combining tag, visual, and aural similarities\n",
    "# with equal weights (1/3 each). The hybrid similarity will be applied only when there \n",
    "# are 5 or fewer common raters (L1 = 5).\n",
    "\n",
    "sim_hybrid_matrix = hybrid_sim(5, 1/3, 1/3, 1/3)\n",
    "generate_dat_file_sim(sim_hybrid_matrix, \"sim_hybrid_matrix\")\n",
    "ItemAttributeKNN('train.dat', 'test.dat', similarity_file='sim_hybrid_matrix.dat', as_similar_first=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79379c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Rating Prediction > Item Attribute KNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 5.092627 sec\n",
      "prediction_time:: 0.833756 sec\n",
      "Eval:: MAE: 0.732889 RMSE: 0.964418 \n"
     ]
    }
   ],
   "source": [
    "# Test 6 - Hybrid Similarity with L1 = 100 and Equal Weights \n",
    "# (L1 = 100, w_tag, w_visual, w_aural) = (100, 1/3, 1/3, 1/3)\n",
    "# This test will use a hybrid similarity matrix combining tag, visual, and aural similarities\n",
    "# with equal weights (1/3 each). The hybrid similarity will be applied only when there \n",
    "# are 100 or fewer common raters (L1 = 100).\n",
    "\n",
    "sim_hybrid_matrix_100 = hybrid_sim(100, 1/3, 1/3, 1/3)\n",
    "generate_dat_file_sim(sim_hybrid_matrix_100, \"sim_hybrid_matrix_100\")\n",
    "ItemAttributeKNN('train.dat', 'test.dat', similarity_file='sim_hybrid_matrix_100.dat', as_similar_first=True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c9b0b",
   "metadata": {},
   "source": [
    "***Exercício 02:*** Vamos implementar um recomendador híbrido canalizado em cascata, no cenário de ranqueamento. A ideia é que um primeiro algoritmo gere uma lista C1 de N=50 itens candidatos à recomendação para cada usuário. Em seguida, um outro recomendador irá gerar uma outra lista C2 também de N=50 itens candidatos à rcomendação para cada usuário. Por fim, o ranking final será a intersecção entre C1 e C2, sendo o score de cada itens formado pela média aritmética dos scores de cada lista. Avalie o desempenho.\n",
    "\n",
    "Dica 1: utilize o parâmetro rank_length disponível nos algoritmos de ranqueamento do CaseRecommender para especificar o tamanho N de recomendações para cada usuário.\n",
    "\n",
    "Dica 2: você pode gravar num arquivo os rankings gerados por um algoritmo para cada usuário especificando o nome do arquivo no parâmetro output_file.\n",
    "\n",
    "Dica 3: consulte a Aula 04 que contém algumas métricas de avaliação de ranqueamento. Como você irá gerar o ranking final externamente ao CaseRecommender, será necessário avaliá-lo usando funções próprias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "225338fd-575e-431e-8efc-33dfca229966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Case Recommender: Item Recommendation > ItemKNN Algorithm]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 1.314007 sec\n",
      "prediction_time:: 48.040426 sec\n",
      "\n",
      "\n",
      "Eval:: PREC@1: 0.422193 PREC@3: 0.307003 PREC@5: 0.252521 PREC@10: 0.187201 RECALL@1: 0.138504 RECALL@3: 0.285647 RECALL@5: 0.382874 RECALL@10: 0.552056 MAP@1: 0.422193 MAP@3: 0.514284 MAP@5: 0.517983 MAP@10: 0.489237 NDCG@1: 0.422193 NDCG@3: 0.602418 NDCG@5: 0.620641 NDCG@10: 0.614089 \n",
      "[Case Recommender: Item Recommendation > BPRMF]\n",
      "\n",
      "train data:: 11090 users and 405 items (152496 interactions) | sparsity:: 96.60%\n",
      "test data:: 10571 users and 331 items (38125 interactions) | sparsity:: 98.91%\n",
      "\n",
      "training_time:: 131.841009 sec\n",
      "prediction_time:: 2.876345 sec\n",
      "\n",
      "\n",
      "Eval:: PREC@1: 0.367515 PREC@3: 0.260366 PREC@5: 0.220263 PREC@10: 0.169284 RECALL@1: 0.118391 RECALL@3: 0.238812 RECALL@5: 0.330738 RECALL@10: 0.495048 MAP@1: 0.367515 MAP@3: 0.45237 MAP@5: 0.459143 MAP@10: 0.437381 NDCG@1: 0.367515 NDCG@3: 0.533378 NDCG@5: 0.558789 NDCG@10: 0.561644 \n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# The models that will be used are ItemKNN and BbrMF\n",
    "ItemKNN('train.dat', 'test.dat', 'ir_itemknn.dat', rank_length=50).compute()\n",
    "BprMF('train.dat', 'test.dat', 'ir_bprmf.dat', factors=3, rank_length=50).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48e10871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score: 0.4612911381539892\n"
     ]
    }
   ],
   "source": [
    "# Candidates movies to be recommended to each user\n",
    "C1 = pd.read_csv('./ir_itemknn.dat', sep='\\t', names=['userId', 'movieId', 'score'])\n",
    "C2 = pd.read_csv('./ir_bprmf.dat', sep='\\t', names=['userId', 'movieId', 'score'])\n",
    "\n",
    "def recommended_movies(user_id, list):\n",
    "    if list == 'C1':\n",
    "        # Filter C1 in order to get the recommendations for a specific user\n",
    "        return C1.loc[C1.userId == user_id, ['movieId', 'score']].set_index('movieId')['score'].to_dict()\n",
    "    elif list == 'C2':\n",
    "        # Filter C2 in order to get the recommendations for a specific user\n",
    "        return C2.loc[C2.userId == user_id, ['movieId', 'score']].set_index('movieId')['score'].to_dict()\n",
    "\n",
    "# Combine the recommendations of C1 and C2\n",
    "def combine_recommendations(user_id):\n",
    "    rec_c1 = recommended_movies(user_id, 'C1')\n",
    "    rec_c2 = recommended_movies(user_id, 'C2')\n",
    "    \n",
    "    # Find the intersection of the recommended movies\n",
    "    common_movies = set(rec_c1.keys()).intersection(set(rec_c2.keys()))\n",
    "    \n",
    "    # Calculate the scores mean\n",
    "    combined_scores = {}\n",
    "    for movie in common_movies:\n",
    "        combined_scores[movie] = (rec_c1[movie] + rec_c2[movie]) / 2\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "# Create a dataframe for the combined recommendations\n",
    "user_ids = C1['userId'].unique()\n",
    "combined_recommendations = []\n",
    "\n",
    "for user_id in user_ids:\n",
    "    combined_rec = combine_recommendations(user_id)\n",
    "    for movie_id, score in combined_rec.items():\n",
    "        combined_recommendations.append({'userId': user_id, 'movieId': movie_id, 'score': score})\n",
    "\n",
    "combined_recommendations_df = pd.DataFrame(combined_recommendations)\n",
    "combined_recommendations_df = combined_recommendations_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Calculate the average precision for a user\n",
    "def average_precision(user_id, recommendations, test_set, limit): \n",
    "    # Get the recommendations for the specific user\n",
    "    recs_user = recommendations.loc[(recommendations.userId == user_id), 'movieId'].tolist()\n",
    "    # Get the relevant movies for the specific user\n",
    "    relevant_movies = test_set.loc[(test_set.userId == user_id), 'movieId'].tolist()\n",
    "    \n",
    "    n_relevant_movies = 0\n",
    "    cumulative_precision = 0.0\n",
    "    total = 0\n",
    "    # Iterate over the recommendations and calculate the precision\n",
    "    for i, movie in enumerate(recs_user):\n",
    "        total += 1\n",
    "        if movie in relevant_movies:\n",
    "            n_relevant_movies += 1\n",
    "            # Precision at index i\n",
    "            precision_at_i = n_relevant_movies / (i + 1)\n",
    "            cumulative_precision += precision_at_i\n",
    "        if total == limit:\n",
    "            break\n",
    "    if n_relevant_movies == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    ap = cumulative_precision / n_relevant_movies\n",
    "    return ap\n",
    "\n",
    "# Calculate the mean average precision\n",
    "def mean_average_precision(recommendations, test_set, limit=10):\n",
    "    average_precisions = []\n",
    "    \n",
    "    # Get all unique users Ids\n",
    "    unique_users = recommendations['userId'].unique()\n",
    "    \n",
    "    for user_id in unique_users:\n",
    "        ap = average_precision(user_id, recommendations, test_set, limit)\n",
    "        average_precisions.append(ap)\n",
    "    \n",
    "    return np.mean(average_precisions)\n",
    "    \n",
    "# Calculate the MAP score\n",
    "map_score = mean_average_precision(combined_recommendations_df, test)\n",
    "print(f'MAP Score: {map_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4052d-1bb6-4341-87dd-70698e78449d",
   "metadata": {},
   "source": [
    "O modelo híbrido conseguiu melhorar o desempenho em relação ao BPRMF, mas não foi capaz de superar o ItemKNN, apesar dos resultados relativamente semelhantes da metrica MAP@10. Isso sugere que o ganho ao combinar esses dois métodos foi limitado, provavelmente porque o ItemKNN já captura a maioria dos padrões relevantes do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7ba9e-a798-4980-9cba-422b00a615cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
